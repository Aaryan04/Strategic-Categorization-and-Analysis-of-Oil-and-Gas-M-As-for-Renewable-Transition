{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (4.41.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (2.3.1)\n",
      "Collecting torch\n",
      "  Downloading torch-2.4.0-cp39-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.0-cp39-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.1/62.1 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch, transformers\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.1\n",
      "    Uninstalling torch-2.3.1:\n",
      "      Successfully uninstalled torch-2.3.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.41.2\n",
      "    Uninstalling transformers-4.41.2:\n",
      "      Successfully uninstalled transformers-4.41.2\n",
      "Successfully installed torch-2.4.0 transformers-4.44.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a CUDA-enabled GPU is available or if using MPS (for macOS with M1/M2 chip)\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "use_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Determine if CUDA or MPS is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aaryanshah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96b64154fe64155a1b9a52c00eecbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/889 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6019, 'grad_norm': 3.533015251159668, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.39}\n",
      "{'loss': 1.3696, 'grad_norm': 5.096282482147217, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a3f6c3efa0446bb55c54a9f6e4b447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0518385171890259, 'eval_accuracy': 0.6094674556213018, 'eval_f1': 0.46158197006613294, 'eval_runtime': 28.4526, 'eval_samples_per_second': 17.819, 'eval_steps_per_second': 1.125, 'epoch': 1.0}\n",
      "{'loss': 1.0785, 'grad_norm': 7.578855037689209, 'learning_rate': 6e-06, 'epoch': 1.18}\n",
      "{'loss': 0.8371, 'grad_norm': 8.223987579345703, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.57}\n",
      "{'loss': 0.7646, 'grad_norm': 7.4581403732299805, 'learning_rate': 1e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd58367116f240a8a0056a1e0e2d8e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7563526034355164, 'eval_accuracy': 0.7159763313609467, 'eval_f1': 0.6845190133348386, 'eval_runtime': 24.159, 'eval_samples_per_second': 20.986, 'eval_steps_per_second': 1.325, 'epoch': 2.0}\n",
      "{'loss': 0.7301, 'grad_norm': 19.088003158569336, 'learning_rate': 1.2e-05, 'epoch': 2.36}\n",
      "{'loss': 0.6682, 'grad_norm': 9.154130935668945, 'learning_rate': 1.4e-05, 'epoch': 2.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc7d814a3cc45e9ab0059f8685cdca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7014633417129517, 'eval_accuracy': 0.7396449704142012, 'eval_f1': 0.7207743851837118, 'eval_runtime': 25.498, 'eval_samples_per_second': 19.884, 'eval_steps_per_second': 1.255, 'epoch': 3.0}\n",
      "{'loss': 0.6005, 'grad_norm': 11.35035514831543, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.15}\n",
      "{'loss': 0.5792, 'grad_norm': 11.842965126037598, 'learning_rate': 1.8e-05, 'epoch': 3.54}\n",
      "{'loss': 0.5879, 'grad_norm': 10.356588363647461, 'learning_rate': 2e-05, 'epoch': 3.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05b3d96fc36407086052892d900451a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7531223297119141, 'eval_accuracy': 0.7435897435897436, 'eval_f1': 0.7312949024982321, 'eval_runtime': 23.427, 'eval_samples_per_second': 21.642, 'eval_steps_per_second': 1.366, 'epoch': 4.0}\n",
      "{'loss': 0.4855, 'grad_norm': 27.87413215637207, 'learning_rate': 1.74293059125964e-05, 'epoch': 4.33}\n",
      "{'loss': 0.5421, 'grad_norm': 9.233284950256348, 'learning_rate': 1.4858611825192803e-05, 'epoch': 4.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7444c534cd2144d2b195737f9c1f2de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7131755948066711, 'eval_accuracy': 0.73767258382643, 'eval_f1': 0.7393242515753313, 'eval_runtime': 23.9998, 'eval_samples_per_second': 21.125, 'eval_steps_per_second': 1.333, 'epoch': 5.0}\n",
      "{'loss': 0.3933, 'grad_norm': 4.338791847229004, 'learning_rate': 1.2287917737789203e-05, 'epoch': 5.12}\n",
      "{'loss': 0.3486, 'grad_norm': 35.08097839355469, 'learning_rate': 9.717223650385606e-06, 'epoch': 5.51}\n",
      "{'loss': 0.37, 'grad_norm': 11.659290313720703, 'learning_rate': 7.1465295629820055e-06, 'epoch': 5.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c45607049fe40d4a4530c35be886b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7039790153503418, 'eval_accuracy': 0.7633136094674556, 'eval_f1': 0.7661079350686514, 'eval_runtime': 24.2307, 'eval_samples_per_second': 20.924, 'eval_steps_per_second': 1.321, 'epoch': 6.0}\n",
      "{'loss': 0.2947, 'grad_norm': 14.355135917663574, 'learning_rate': 4.575835475578407e-06, 'epoch': 6.3}\n",
      "{'loss': 0.2456, 'grad_norm': 11.556453704833984, 'learning_rate': 2.0051413881748076e-06, 'epoch': 6.69}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e5546d60694a47a5584acb2df2cb7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7707707285881042, 'eval_accuracy': 0.7652859960552268, 'eval_f1': 0.7664964720517852, 'eval_runtime': 23.2276, 'eval_samples_per_second': 21.827, 'eval_steps_per_second': 1.378, 'epoch': 7.0}\n",
      "{'train_runtime': 2819.8115, 'train_samples_per_second': 5.027, 'train_steps_per_second': 0.315, 'train_loss': 0.6582898235428319, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b28340dfdb46be956f8c657f7888da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Accuracy: 0.7396449704142012\n",
      "Fold 1 - F1-Score: 0.7207743851837118\n",
      "\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54de19b2bb540a88bdb2f9d0cd40ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/889 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5946, 'grad_norm': 3.4140305519104004, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.39}\n",
      "{'loss': 1.3652, 'grad_norm': 7.586578845977783, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f596518953941c1afd901b2805d1807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1705008745193481, 'eval_accuracy': 0.6094674556213018, 'eval_f1': 0.46158197006613294, 'eval_runtime': 24.3335, 'eval_samples_per_second': 20.835, 'eval_steps_per_second': 1.315, 'epoch': 1.0}\n",
      "{'loss': 1.1933, 'grad_norm': 14.963207244873047, 'learning_rate': 6e-06, 'epoch': 1.18}\n",
      "{'loss': 0.8945, 'grad_norm': 7.3716139793396, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.57}\n",
      "{'loss': 0.8189, 'grad_norm': 7.62677526473999, 'learning_rate': 1e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36d809d2c774999aada852fba2042d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8117831945419312, 'eval_accuracy': 0.7080867850098619, 'eval_f1': 0.6770593816758359, 'eval_runtime': 23.8576, 'eval_samples_per_second': 21.251, 'eval_steps_per_second': 1.341, 'epoch': 2.0}\n",
      "{'loss': 0.7489, 'grad_norm': 22.392475128173828, 'learning_rate': 1.2e-05, 'epoch': 2.36}\n",
      "{'loss': 0.6931, 'grad_norm': 5.736989974975586, 'learning_rate': 1.4e-05, 'epoch': 2.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d1a0298d604d02a7d78a40b50f2df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.718282163143158, 'eval_accuracy': 0.7218934911242604, 'eval_f1': 0.7299347514499277, 'eval_runtime': 24.0806, 'eval_samples_per_second': 21.054, 'eval_steps_per_second': 1.329, 'epoch': 3.0}\n",
      "{'loss': 0.6495, 'grad_norm': 13.466558456420898, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.15}\n",
      "{'loss': 0.5851, 'grad_norm': 25.43783187866211, 'learning_rate': 1.8e-05, 'epoch': 3.54}\n",
      "{'loss': 0.6418, 'grad_norm': 9.57933521270752, 'learning_rate': 2e-05, 'epoch': 3.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820d1ed8510c4549b406769b807d81a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6371483206748962, 'eval_accuracy': 0.7435897435897436, 'eval_f1': 0.7524888197694655, 'eval_runtime': 23.5035, 'eval_samples_per_second': 21.571, 'eval_steps_per_second': 1.361, 'epoch': 4.0}\n",
      "{'loss': 0.4856, 'grad_norm': 20.007740020751953, 'learning_rate': 1.74293059125964e-05, 'epoch': 4.33}\n",
      "{'loss': 0.4851, 'grad_norm': 4.887363433837891, 'learning_rate': 1.4858611825192803e-05, 'epoch': 4.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc7c2883d2240028fbf59c6b3dc6699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6993793249130249, 'eval_accuracy': 0.7554240631163708, 'eval_f1': 0.7533253600442557, 'eval_runtime': 23.1543, 'eval_samples_per_second': 21.897, 'eval_steps_per_second': 1.382, 'epoch': 5.0}\n",
      "{'loss': 0.4791, 'grad_norm': 12.985546112060547, 'learning_rate': 1.2287917737789203e-05, 'epoch': 5.12}\n",
      "{'loss': 0.305, 'grad_norm': 13.249201774597168, 'learning_rate': 9.717223650385606e-06, 'epoch': 5.51}\n",
      "{'loss': 0.3407, 'grad_norm': 12.86470890045166, 'learning_rate': 7.1465295629820055e-06, 'epoch': 5.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210de793502f4f0d9c4f8684e48716fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.762715220451355, 'eval_accuracy': 0.7672583826429981, 'eval_f1': 0.7684960315361515, 'eval_runtime': 22.7268, 'eval_samples_per_second': 22.308, 'eval_steps_per_second': 1.408, 'epoch': 6.0}\n",
      "{'loss': 0.2607, 'grad_norm': 18.80027198791504, 'learning_rate': 4.575835475578407e-06, 'epoch': 6.3}\n",
      "{'loss': 0.2082, 'grad_norm': 10.20966911315918, 'learning_rate': 2.0051413881748076e-06, 'epoch': 6.69}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc136f29e50f425fb6572f5ada0308a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8170561790466309, 'eval_accuracy': 0.7593688362919132, 'eval_f1': 0.7568416532285164, 'eval_runtime': 22.595, 'eval_samples_per_second': 22.439, 'eval_steps_per_second': 1.416, 'epoch': 7.0}\n",
      "{'train_runtime': 2391.3036, 'train_samples_per_second': 5.928, 'train_steps_per_second': 0.372, 'train_loss': 0.6728727868491002, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d071b015bd0b4bbc94409bde6e0dd18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Accuracy: 0.7435897435897436\n",
      "Fold 2 - F1-Score: 0.7524888197694655\n",
      "\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7baff9b69b984b0e8001412ef42a79cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/889 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5977, 'grad_norm': 2.7441699504852295, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.39}\n",
      "{'loss': 1.4025, 'grad_norm': 8.70174503326416, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d77532c9304fe286c3511c910f8894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1527999639511108, 'eval_accuracy': 0.6126482213438735, 'eval_f1': 0.4654925211191196, 'eval_runtime': 22.6165, 'eval_samples_per_second': 22.373, 'eval_steps_per_second': 1.415, 'epoch': 1.0}\n",
      "{'loss': 1.1742, 'grad_norm': 4.826243877410889, 'learning_rate': 6e-06, 'epoch': 1.18}\n",
      "{'loss': 0.8451, 'grad_norm': 8.929620742797852, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.57}\n",
      "{'loss': 0.7681, 'grad_norm': 11.2863130569458, 'learning_rate': 1e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6060562add9447518697263e59e9de86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7418118119239807, 'eval_accuracy': 0.7450592885375494, 'eval_f1': 0.7268291407690205, 'eval_runtime': 26.4641, 'eval_samples_per_second': 19.12, 'eval_steps_per_second': 1.209, 'epoch': 2.0}\n",
      "{'loss': 0.7062, 'grad_norm': 17.02804946899414, 'learning_rate': 1.2e-05, 'epoch': 2.36}\n",
      "{'loss': 0.6479, 'grad_norm': 7.491434097290039, 'learning_rate': 1.4e-05, 'epoch': 2.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291416399ee84b29a4f5b813ff401c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6957205533981323, 'eval_accuracy': 0.7509881422924901, 'eval_f1': 0.71967013971288, 'eval_runtime': 68.0844, 'eval_samples_per_second': 7.432, 'eval_steps_per_second': 0.47, 'epoch': 3.0}\n",
      "{'loss': 0.6394, 'grad_norm': 6.098319053649902, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.15}\n",
      "{'loss': 0.6078, 'grad_norm': 11.340021133422852, 'learning_rate': 1.8e-05, 'epoch': 3.54}\n",
      "{'loss': 0.5915, 'grad_norm': 12.52563190460205, 'learning_rate': 2e-05, 'epoch': 3.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f51fdd898b84a3b884b2c05bd749cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6626803874969482, 'eval_accuracy': 0.7628458498023716, 'eval_f1': 0.7496164792013364, 'eval_runtime': 29.7952, 'eval_samples_per_second': 16.983, 'eval_steps_per_second': 1.074, 'epoch': 4.0}\n",
      "{'loss': 0.5135, 'grad_norm': 27.625699996948242, 'learning_rate': 1.74293059125964e-05, 'epoch': 4.33}\n",
      "{'loss': 0.5341, 'grad_norm': 9.56180477142334, 'learning_rate': 1.4858611825192803e-05, 'epoch': 4.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ed0577e4df44318c5ab00fec7c1968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7120953798294067, 'eval_accuracy': 0.7648221343873518, 'eval_f1': 0.7615529514068896, 'eval_runtime': 27.5241, 'eval_samples_per_second': 18.384, 'eval_steps_per_second': 1.163, 'epoch': 5.0}\n",
      "{'loss': 0.4362, 'grad_norm': 8.148985862731934, 'learning_rate': 1.2287917737789203e-05, 'epoch': 5.12}\n",
      "{'loss': 0.395, 'grad_norm': 14.570697784423828, 'learning_rate': 9.717223650385606e-06, 'epoch': 5.51}\n",
      "{'loss': 0.2988, 'grad_norm': 12.05713939666748, 'learning_rate': 7.1465295629820055e-06, 'epoch': 5.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f36dab268384c95aaa4bb72617adabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.807100772857666, 'eval_accuracy': 0.7430830039525692, 'eval_f1': 0.7367651621583413, 'eval_runtime': 30.9542, 'eval_samples_per_second': 16.347, 'eval_steps_per_second': 1.034, 'epoch': 6.0}\n",
      "{'loss': 0.2943, 'grad_norm': 10.870992660522461, 'learning_rate': 4.575835475578407e-06, 'epoch': 6.3}\n",
      "{'loss': 0.2452, 'grad_norm': 15.95904541015625, 'learning_rate': 2.0051413881748076e-06, 'epoch': 6.69}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39e8bd52c1e4fd794ddf70e21844b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.806674063205719, 'eval_accuracy': 0.7490118577075099, 'eval_f1': 0.7466997785126545, 'eval_runtime': 38.1888, 'eval_samples_per_second': 13.25, 'eval_steps_per_second': 0.838, 'epoch': 7.0}\n",
      "{'train_runtime': 5242.2085, 'train_samples_per_second': 2.705, 'train_steps_per_second': 0.17, 'train_loss': 0.6684219416134537, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9d4ca167f94058922d963f0fcf02c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Accuracy: 0.7628458498023716\n",
      "Fold 3 - F1-Score: 0.7496164792013364\n",
      "\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750e7242ff834d4988c85cf5a695260e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/889 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5952, 'grad_norm': 5.519248962402344, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.39}\n",
      "{'loss': 1.3631, 'grad_norm': 6.604671001434326, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e363efa78e94b9aa0e26ff9e35a0b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1483759880065918, 'eval_accuracy': 0.6126482213438735, 'eval_f1': 0.4654925211191196, 'eval_runtime': 22.6517, 'eval_samples_per_second': 22.338, 'eval_steps_per_second': 1.413, 'epoch': 1.0}\n",
      "{'loss': 1.1414, 'grad_norm': 12.80813217163086, 'learning_rate': 6e-06, 'epoch': 1.18}\n",
      "{'loss': 0.8981, 'grad_norm': 10.222712516784668, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.57}\n",
      "{'loss': 0.7806, 'grad_norm': 6.732129096984863, 'learning_rate': 1e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7739ae35683048499162ffb7f5f619b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aaryanshah/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7823296785354614, 'eval_accuracy': 0.7272727272727273, 'eval_f1': 0.7192996340441216, 'eval_runtime': 341.3288, 'eval_samples_per_second': 1.482, 'eval_steps_per_second': 0.094, 'epoch': 2.0}\n",
      "{'loss': 0.6856, 'grad_norm': 13.266212463378906, 'learning_rate': 1.2e-05, 'epoch': 2.36}\n",
      "{'loss': 0.7135, 'grad_norm': 10.422714233398438, 'learning_rate': 1.4e-05, 'epoch': 2.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134e9df66d064662941f7308a6f90f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7482283115386963, 'eval_accuracy': 0.7292490118577075, 'eval_f1': 0.7238912493861531, 'eval_runtime': 23.2897, 'eval_samples_per_second': 21.726, 'eval_steps_per_second': 1.374, 'epoch': 3.0}\n",
      "{'loss': 0.6302, 'grad_norm': 11.530143737792969, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.15}\n",
      "{'loss': 0.5706, 'grad_norm': 22.487537384033203, 'learning_rate': 1.8e-05, 'epoch': 3.54}\n",
      "{'loss': 0.5901, 'grad_norm': 29.152694702148438, 'learning_rate': 2e-05, 'epoch': 3.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1be9aae24f04dc2aca4331c687f7fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaryanshah/Oncampus-Job/NLP_Gal/prediction/try2.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aaryanshah/Oncampus-Job/NLP_Gal/prediction/try2.ipynb#W2sZmlsZQ%3D%3D?line=322'>323</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aaryanshah/Oncampus-Job/NLP_Gal/prediction/try2.ipynb#W2sZmlsZQ%3D%3D?line=323'>324</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aaryanshah/Oncampus-Job/NLP_Gal/prediction/try2.ipynb#W2sZmlsZQ%3D%3D?line=324'>325</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aaryanshah/Oncampus-Job/NLP_Gal/prediction/try2.ipynb#W2sZmlsZQ%3D%3D?line=331'>332</a>\u001b[0m     callbacks\u001b[39m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aaryanshah/Oncampus-Job/NLP_Gal/prediction/try2.ipynb#W2sZmlsZQ%3D%3D?line=332'>333</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aaryanshah/Oncampus-Job/NLP_Gal/prediction/try2.ipynb#W2sZmlsZQ%3D%3D?line=334'>335</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/aaryanshah/Oncampus-Job/NLP_Gal/prediction/try2.ipynb#W2sZmlsZQ%3D%3D?line=335'>336</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aaryanshah/Oncampus-Job/NLP_Gal/prediction/try2.ipynb#W2sZmlsZQ%3D%3D?line=337'>338</a>\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aaryanshah/Oncampus-Job/NLP_Gal/prediction/try2.ipynb#W2sZmlsZQ%3D%3D?line=338'>339</a>\u001b[0m eval_result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:1948\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1946\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1947\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1948\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1949\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1950\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1951\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1952\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1953\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:2386\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2383\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2385\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 2386\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   2388\u001b[0m \u001b[39mif\u001b[39;00m DebugOption\u001b[39m.\u001b[39mTPU_METRICS_DEBUG \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdebug:\n\u001b[1;32m   2389\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2390\u001b[0m         \u001b[39m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:2814\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2812\u001b[0m metrics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2814\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate(trial, ignore_keys_for_eval)\n\u001b[1;32m   2816\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n\u001b[1;32m   2817\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[39m=\u001b[39mmetrics)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:2771\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2770\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_evaluate\u001b[39m(\u001b[39mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m-> 2771\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[1;32m   2772\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2774\u001b[0m     \u001b[39m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:3676\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3673\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3675\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3676\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   3677\u001b[0m     eval_dataloader,\n\u001b[1;32m   3678\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   3679\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3680\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3681\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   3682\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   3683\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   3684\u001b[0m )\n\u001b[1;32m   3686\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   3687\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:3867\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3864\u001b[0m         batch_size \u001b[39m=\u001b[39m observed_batch_size\n\u001b[1;32m   3866\u001b[0m \u001b[39m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3867\u001b[0m losses, logits, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprediction_step(model, inputs, prediction_loss_only, ignore_keys\u001b[39m=\u001b[39;49mignore_keys)\n\u001b[1;32m   3868\u001b[0m main_input_name \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39m\"\u001b[39m\u001b[39mmain_input_name\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3869\u001b[0m inputs_decode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39minclude_inputs_for_metrics \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:4085\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4083\u001b[0m \u001b[39mif\u001b[39;00m has_labels \u001b[39mor\u001b[39;00m loss_without_labels:\n\u001b[1;32m   4084\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4085\u001b[0m         loss, outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs, return_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   4086\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m   4088\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:3373\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3371\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3372\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 3373\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   3374\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3375\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/aaryanshah/Oncampus-Job/NLP_Gal/prediction/try2.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aaryanshah/Oncampus-Job/NLP_Gal/prediction/try2.ipynb#W2sZmlsZQ%3D%3D?line=258'>259</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aaryanshah/Oncampus-Job/NLP_Gal/prediction/try2.ipynb#W2sZmlsZQ%3D%3D?line=259'>260</a>\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/aaryanshah/Oncampus-Job/NLP_Gal/prediction/try2.ipynb#W2sZmlsZQ%3D%3D?line=260'>261</a>\u001b[0m     custom_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m1.0\u001b[39;49m, \u001b[39m1.2\u001b[39;49m, \u001b[39m1.2\u001b[39;49m, \u001b[39m1.5\u001b[39;49m, \u001b[39m2.0\u001b[39;49m], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat)\u001b[39m.\u001b[39;49mto(logits\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aaryanshah/Oncampus-Job/NLP_Gal/prediction/try2.ipynb#W2sZmlsZQ%3D%3D?line=261'>262</a>\u001b[0m     loss_fct \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss(weight\u001b[39m=\u001b[39mcustom_weights)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aaryanshah/Oncampus-Job/NLP_Gal/prediction/try2.ipynb#W2sZmlsZQ%3D%3D?line=262'>263</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fct(logits\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_labels), labels\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "import torch.nn.functional as F\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "# Download NLTK stop words\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/Users/aaryanshah/Oncampus-Job/NLP_Gal/data/TrainingSet 1(Deals).csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define text columns and the target column\n",
    "text_columns = ['Target Business Description', 'M&A Headline', 'Deal Synopsis']\n",
    "df['combined_text'] = df[text_columns].apply(lambda x: ' '.join(x.dropna()), axis=1)\n",
    "\n",
    "# Clean the text data\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['combined_text'].apply(clean_text)\n",
    "\n",
    "# Define score columns\n",
    "score_columns = ['Singh', 'Arora', 'Neilsen', 'RH', 'Uche', 'Edrick']\n",
    "\n",
    "# Function to remove the outlier from each row using manual iteration to find the max difference\n",
    "def remove_outlier_manual(row):\n",
    "    scores = row[score_columns].dropna()  # Remove NaNs for processing\n",
    "    if len(scores) > 1:  # Ensure there are enough scores to compare\n",
    "        median = np.median(scores)\n",
    "        diffs = np.abs(scores - median)\n",
    "        max_diff_value = diffs.max()\n",
    "        max_diff_index = diffs[diffs == max_diff_value].index[0]\n",
    "        row.at[max_diff_index] = np.nan\n",
    "    return row\n",
    "\n",
    "# Apply the outlier removal function\n",
    "df_cleaned = df.apply(remove_outlier_manual, axis=1)\n",
    "\n",
    "# Calculate the average for each row\n",
    "df_cleaned['Average'] = df_cleaned[score_columns].mean(axis=1)\n",
    "\n",
    "# Handle NaN values in the Average column\n",
    "df_cleaned = df_cleaned.dropna(subset=['Average'])\n",
    "\n",
    "# Ensure 'Average' values are within 1 to 5\n",
    "df_cleaned = df_cleaned[df_cleaned['Average'].between(1, 5)]\n",
    "\n",
    "# Map the 'average' scores to ensure they are within 1 to 5 range\n",
    "target_column = 'Average'\n",
    "df_cleaned['labels'] = df_cleaned[target_column].astype(int) - 1  # Make sure labels are 0-indexed for classification\n",
    "\n",
    "\n",
    "# Additional Domain-Specific Keyword Lists\n",
    "core_keywords = set([\n",
    "    \"crude\", \"shale\", \"hydrocarbon\", \"methane\", \"petroleum\", \"gas\", \"oil\",\n",
    "    \"gasoline\", \"diesel\", \"ethanol\", \"propane\", \"petrol\", \"coke\", \"gasohol\",\n",
    "    \"coal\", \"lng\", \"barrel\", \"condensate\", \"benzene\", \"bitumen\", \"asphalt\",\n",
    "    \"coking\", \"butane\", \"kerosene\", \"bopd\", \"butene\", \"fossil\", \"liqufied\",\n",
    "    \"dlng\", \"fuel\", \"engine\", \"oilgas\", \"petrolium\", \"petroli\", \"petroleos\",\n",
    "    \"gasrelated\", \"gasrec\", \"petroluem\", \"petrolera\", \"gallon\", \"gasfueled\",\n",
    "    \"gasinvest\", \"water\", \"fluid\", \"liquide\", \"grease\", \"liquor\", \"rock\",\n",
    "    \"gasket\", \"condes\", \"octanex\", \"toilet\", \"carbide\", \"catalytic\", \"vapor\",\n",
    "    \"coolant\", \"boilers\", \"hydrogenfuelled\", \"lignite\", \"pine\", \"pollution\",\n",
    "    \"pygas\", \"mortar\", \"exxon\", \"exxonmobil\", \"chevron\", \"cement\", \"waste\",\n",
    "    \"concrete\", \"hydrogen\", \"liquefied\", \"hydrothermal\", \"liquid\", \"sewage\",\n",
    "    \"plastic\", \"polypropylene\", \"polyethylene\", \"polymer\", \"propylene\",\n",
    "    \"polystyrene\", \"polyester\", \"copolymer\", \"polyvinyl\", \"xylene\", \"polysterene\",\n",
    "    \"polyurethane\", \"polyethlyene\", \"polystyrol\", \"polythene\", \"polyprophylene\",\n",
    "    \"polythylene\", \"polybutylene\", \"polyblend\", \"petrolube\", \"corplubricants\",\n",
    "    \"petroquimica\", \"chemical\", \"lubricant\", \"petrochemical\", \"compound\",\n",
    "    \"pharmaceutical\", \"fertilizer\", \"ethylene\", \"pvc\", \"additive\", \"catalyst\",\n",
    "    \"lubricating\", \"olefin\", \"lube\", \"butadiene\", \"aromatics\", \"thermoplastic\",\n",
    "    \"textile\", \"solvent\", \"thiochemicals\", \"bauxite\", \"mtbe\", \"acetylene\",\n",
    "    \"oleochemicals\", \"acetate\", \"ammonia\", \"ammonium\", \"aromatic\", \"bicarbonate\",\n",
    "    \"biobutanol\", \"chlorochemicals\", \"ethylbenzene\", \"fertiliser\", \"herbicide\",\n",
    "    \"isophtheric\", \"isopropyl\", \"oleochemical\", \"mbpd\", \"petrochemistry\",\n",
    "    \"phosphate\", \"phosphorus\", \"butadine\", \"cfc\", \"lubricants\", \"olefins\",\n",
    "    \"polymerization\", \"pyrolysis\", \"chemicals\", \"inorganic\", \"organic\",\n",
    "    \"synthetic\", \"composite\", \"lubricated\", \"hydrated\", \"hydrochloric\",\n",
    "    \"nitrogenous\", \"reaction\", \"drug\", \"hydrate\", \"foam\", \"synthesis\",\n",
    "    \"chemistry\", \"hydrogenrich\", \"minerales\", \"contaminant\", \"mineral\",\n",
    "    \"uranium\", \"silicate\", \"production\", \"manufacture\", \"produce\",\n",
    "    \"construction\", \"distributor\", \"marketing\", \"supply\", \"building\",\n",
    "    \"producing\", \"manufacturing\", \"trading\", \"advertising\", \"supplying\",\n",
    "    \"constructing\", \"selling\", \"fabrication\", \"purchasing\", \"procurement\",\n",
    "    \"sourcing\", \"develop\", \"developing\", \"sell\", \"maintenance\", \"consulting\",\n",
    "    \"handling\", \"distributing\", \"welding\", \"imaging\", \"repairing\", \"billing\",\n",
    "    \"sales\", \"hr\", \"wholesale\", \"business\", \"manufacturer\", \"industrial\",\n",
    "    \"technology\", \"industry\", \"market\", \"producer\", \"financial\", \"acquisition\",\n",
    "    \"produced\", \"vendor\", \"acquires\", \"revenue\", \"marketed\", \"industriale\",\n",
    "    \"factory\", \"build\", \"stock\", \"purchase\", \"processed\", \"enterprise\",\n",
    "    \"trader\", \"constructed\", \"supplier\", \"industria\", \"making\", \"make\",\n",
    "    \"built\", \"engineered\", \"composed\", \"acquiring\", \"industrials\",\n",
    "    \"corpfilling\", \"produkte\", \"corporate\", \"delivering\", \"development\",\n",
    "    \"equipment\", \"establishment\", \"management\", \"operate\", \"operates\",\n",
    "    \"operating\", \"operation\", \"service\", \"software\", \"services\", \"design\",\n",
    "    \"maintaining\", \"manage\", \"managed\", \"manages\", \"developer\", \"machinery\",\n",
    "    \"repair\", \"commodity\", \"servicing\", \"distributes\", \"specializing\",\n",
    "    \"coordination\", \"designing\", \"distribute\", \"exchange\", \"growth\",\n",
    "    \"ltdmarketing\", \"machining\", \"maintain\", \"managing\", \"operational\",\n",
    "    \"partnership\", \"farming\", \"program\", \"proprietary\", \"woodworking\",\n",
    "    \"deves\", \"strategic\", \"training\", \"assembly\", \"resale\", \"resells\",\n",
    "    \"tech\", \"branded\", \"commercially\", \"contracting\", \"cooperative\",\n",
    "    \"developement\", \"organizing\", \"engineer\", \"finishing\", \"formation\",\n",
    "    \"foundry\", \"programs\", \"quality\", \"operations\", \"planning\", \"productive\",\n",
    "    \"prospecting\", \"merger\", \"inventory\", \"company\", \"gmbhs\", \"llc\", \"corp\",\n",
    "    \"subsidiary\", \"corporation\", \"warehousing\", \"transportation\", \"vehicle\",\n",
    "    \"transport\", \"automotive\", \"delivery\", \"logistics\", \"cargo\", \"car\",\n",
    "    \"shipping\", \"truck\", \"packing\", \"hauling\", \"railway\", \"logistical\",\n",
    "    \"trucking\", \"harbor\", \"seaport\", \"distribution\", \"storage\", \"port\",\n",
    "    \"container\", \"warehouse\", \"bunker\", \"ship\", \"maersk\", \"railroad\",\n",
    "    \"tankers\", \"sea\", \"coast\", \"marine\", \"submarine\", \"maritime\", \"travel\",\n",
    "    \"automobile\", \"transit\", \"passenger\", \"ford\", \"freight\", \"parcel\",\n",
    "    \"bus\", \"drive\", \"truckstop\", \"tow\", \"destination\", \"vehicular\", \"traffic\",\n",
    "    \"towing\", \"transporte\", \"facility\", \"infrastructure\", \"transmission\",\n",
    "    \"packaging\", \"aircraft\", \"courier\", \"subway\", \"trucks\", \"carrier\",\n",
    "    \"deliver\", \"highway\", \"postal\", \"route\", \"drives\", \"airport\", \"place\",\n",
    "    \"stations\", \"airborne\", \"package\", \"tanker\", \"transportadora\",\n",
    "    \"trolley\", \"vessel\", \"carport\", \"dealership\", \"fleet\", \"shipment\",\n",
    "    \"carriers\", \"facilities\", \"cars\", \"landfill\", \"military\", \"crew\",\n",
    "    \"tacoma\", \"airline\", \"offshore\", \"aviation\", \"retail\", \"retailer\",\n",
    "    \"supermarket\", \"shop\", \"shopping\", \"store\", \"station\", \"retailing\",\n",
    "    \"sale\", \"merchant\", \"commerce\", \"customer\", \"wholesaler\", \"dealer\",\n",
    "    \"catalogue\", \"cataloguer\", \"shopsouvenir\", \"refinery\", \"gasification\",\n",
    "    \"distillation\", \"cracking\", \"refueling\", \"fueling\", \"exploration\",\n",
    "    \"mining\", \"refining\", \"engineering\", \"processing\", \"drilling\",\n",
    "    \"midstream\", \"downstream\", \"refine\", \"upstream\", \"extraction\",\n",
    "    \"regasification\", \"refineria\", \"discovery\", \"exploring\", \"pumping\",\n",
    "    \"explore\", \"explores\", \"drill\", \"drilled\", \"refinement\", \"surveying\",\n",
    "    \"sensing\", \"lcm\", \"directional\", \"rotary\", \"blowout\", \"alkylation\",\n",
    "    \"isomerization\", \"stratigraphy\", \"petrology\", \"sedimentology\",\n",
    "    \"stratigraphic\", \"hydrocracking\", \"hydrotreating\", \"hydrodesulfurization\",\n",
    "    \"geology\", \"sagd\", \"dehydrogenation\", \"css\", \"fcc\", \"psa\", \"drainage\",\n",
    "    \"refined\", \"sands\", \"mapping\", \"process\", \"study\", \"hydraulic\", \"flow\",\n",
    "    \"geological\", \"refines\", \"blending\", \"remediation\", \"reprocess\",\n",
    "    \"concentrated\", \"convey\", \"pressure\", \"expertise\", \"processes\",\n",
    "    \"project\", \"studies\", \"gi\", \"limestone\", \"liquefying\", \"plating\",\n",
    "    \"scientific\", \"ground\", \"seismic\", \"sand\", \"pipeline\", \"oilfield\",\n",
    "    \"pipe\", \"rig\", \"reservoir\", \"wellhead\", \"well\", \"pipelines\", \"borehole\",\n",
    "    \"lidar\", \"wellbore\", \"drillships\", \"rigs\", \"bops\", \"eor\", \"mwd\", \"lwd\",\n",
    "    \"ipr\", \"basin\", \"farm\", \"lp\", \"tube\", \"pumped\", \"tubing\", \"inflow\",\n",
    "    \"tank\", \"farms\", \"creek\", \"deepwater\", \"rifle\", \"valve\", \"leak\",\n",
    "    \"cylinder\", \"cavern\", \"coalbed\", \"sink\", \"ductor\", \"finpipe\",\n",
    "    \"shalebased\", \"tanks\", \"pumps\", \"mud\", \"piped\", \"piston\", \"plumbing\",\n",
    "    \"pump\", \"combustion\", \"gasfired\", \"energy\", \"electric\", \"electricity\",\n",
    "    \"mw\", \"battery\", \"turbine\", \"capacity\", \"heating\", \"megawatt\",\n",
    "    \"cogeneration\", \"electrical\", \"grid\", \"generator\", \"gigawatthours\",\n",
    "    \"highvoltage\", \"inverters\", \"turbines\", \"generated\", \"generates\",\n",
    "    \"furnace\", \"unit\", \"heat\", \"thermal\", \"compressor\", \"efficiency\",\n",
    "    \"generating\", \"galvanized\", \"regulator\", \"outlet\", \"exhaust\", \"units\",\n",
    "    \"furnaces\", \"power\", \"inverter\", \"powergen\", \"powertrain\", \"propulsion\",\n",
    "    \"micropower\", \"molten\", \"tesla\", \"emission\", \"reactors\"\n",
    "\n",
    "    # Add more core keywords as necessary...\n",
    "])\n",
    "\n",
    "emerging_keywords = set([\n",
    "    \"solar\", \"photovoltaic\", \"solarbased\", \"solarworld\", \"solarpack\",\n",
    "    \"solares\", \"solarkonzept\", \"sunlight\", \"sunpower\", \"solarbridge\",\n",
    "    \"wind\", \"renewable\", \"renewables\", \"sustainable\", \"hydroelectric\",\n",
    "    \"cleantech\", \"wave\", \"hydro\", \"recycled\", \"lightsource\", \"ecoplastic\",\n",
    "    \"windpower\", \"eco\", \"ecooils\", \"seaenergy\", \"tidal\", \"electrolysis\",\n",
    "    \"otec\", \"ocean\", \"plant\", \"pv\", \"hydros\", \"energetici\", \"natureworks\",\n",
    "    \"terra\", \"atmospheric\", \"cloud\", \"energetica\", \"energeticas\",\n",
    "    \"environment\", \"environmentally\", \"plants\", \"nature\", \"environmental\",\n",
    "    \"nuclear\", \"biofuel\", \"biofuels\", \"bioenergia\", \"biocatalyst\",\n",
    "    \"biological\", \"bioenergy\", \"bio\", \"bioethanol\", \"biosev\",\n",
    "    \"regenerative\", \"bioengineering\", \"biopower\", \"regeneration\",\n",
    "    \"biogaz\", \"biotech\", \"wastetoenergy\", \"bioleo\", \"bioforming\",\n",
    "    \"bioproducts\", \"bioreactor\", \"biochem\", \"biosolutions\", \"biogas\",\n",
    "    \"biomass\", \"biodiesel\", \"sugarcane\", \"biorefineries\", \"biochemical\",\n",
    "    \"biochemtex\", \"biogazowa\", \"bioprocesses\", \"biorefinery\", \"lfg\",\n",
    "    \"biotechnology\", \"botanical\", \"enzyme\"\n",
    "    # Add more emerging keywords as necessary...\n",
    "])\n",
    "\n",
    "# Create Features Based on Keyword Presence\n",
    "def keyword_features(text):\n",
    "    text_set = set(text.split())  # Convert text to a set of words\n",
    "    core_count = len(core_keywords & text_set)\n",
    "    emerging_count = len(emerging_keywords & text_set)\n",
    "    return pd.Series([core_count, emerging_count])\n",
    "\n",
    "df_cleaned[['core_count', 'emerging_count']] = df_cleaned['cleaned_text'].apply(keyword_features)\n",
    "\n",
    "# Pre-tokenize the entire dataset (including additional features)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "max_len = 512\n",
    "tokenized_texts = df_cleaned['cleaned_text'].apply(\n",
    "    lambda x: tokenizer.encode_plus(\n",
    "        x,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    ").tolist()\n",
    "\n",
    "# Convert tokenized data into a custom dataset\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, tokenized_data, labels, core_counts, emerging_counts):\n",
    "        self.input_ids = torch.stack([item['input_ids'].squeeze(0) for item in tokenized_data])\n",
    "        self.attention_mask = torch.stack([item['attention_mask'].squeeze(0) for item in tokenized_data])\n",
    "        self.labels = torch.tensor(labels)\n",
    "        self.core_counts = torch.tensor(core_counts, dtype=torch.float32)\n",
    "        self.emerging_counts = torch.tensor(emerging_counts, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx],\n",
    "            'core_count': self.core_counts[idx],\n",
    "            'emerging_count': self.emerging_counts[idx]\n",
    "        }\n",
    "\n",
    "# Model with modified forward method to use core and emerging keyword counts\n",
    "\n",
    "class CustomRobertaForSequenceClassification(RobertaForSequenceClassification):\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, core_count=None, emerging_count=None):\n",
    "        outputs = super().forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Ensure that labels, core_count, and emerging_count are correctly broadcasted\n",
    "        if labels is not None:\n",
    "            labels_expanded = labels.unsqueeze(-1).expand_as(logits)  # Make sure labels are broadcasted correctly\n",
    "\n",
    "            # Enhance logits adjustments with differentiated weighting based on class performance insights\n",
    "            logits_adjustment_core = 0.1 + 0.05 * (labels_expanded == 4).float() + 0.05 * (labels_expanded == 5).float()\n",
    "            logits_adjustment_emerging = 0.1 + 0.05 * (labels_expanded == 4).float() + 0.05 * (labels_expanded == 5).float()\n",
    "\n",
    "            logits += core_count.unsqueeze(1) * logits_adjustment_core\n",
    "            logits += emerging_count.unsqueeze(1) * logits_adjustment_emerging\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            custom_weights = torch.tensor([1.0, 1.2, 1.2, 1.5, 2.0], dtype=torch.float).to(logits.device)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=custom_weights)\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "\n",
    "\n",
    "# Cross-validation setup\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold = 0\n",
    "\n",
    "# Prepare results storage\n",
    "all_fold_results = []\n",
    "\n",
    "# Training arguments\n",
    "# Modify the TrainingArguments without FP16\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  # Directory for storing logs and model checkpoints\n",
    "    num_train_epochs=7,  # Train for 7 epochs\n",
    "    per_device_train_batch_size=8,  # Smaller batch size to manage GPU memory\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation\n",
    "    warmup_steps=500,  # Number of warmup steps for learning rate scheduler\n",
    "    warmup_ratio=0.1,  # Alternatively, set warmup as a ratio of total steps\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    logging_dir='./logs',  # Directory for storing logs\n",
    "    logging_steps=50,  # Log model predictions every 50 steps\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate model at the end of each epoch\n",
    "    learning_rate=2e-5,  # Higher initial learning rate\n",
    "    lr_scheduler_type='linear',  # Learning rate scheduler\n",
    "    fp16=False,  # Disable mixed precision training\n",
    "    gradient_accumulation_steps=2,  # Use gradient accumulation for larger effective batch size\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\",  # Use evaluation loss to determine the best model\n",
    "    save_total_limit=2  # Limit the total amount of checkpoints; older ones are deleted\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for train_index, test_index in skf.split(df_cleaned['cleaned_text'], df_cleaned['labels']):\n",
    "    fold += 1\n",
    "    print(f\"\\nFold {fold}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = CustomTextDataset(\n",
    "        [tokenized_texts[i] for i in train_index],\n",
    "        df_cleaned['labels'].iloc[train_index].tolist(),\n",
    "        df_cleaned['core_count'].iloc[train_index].tolist(),\n",
    "        df_cleaned['emerging_count'].iloc[train_index].tolist()\n",
    "    )\n",
    "    test_dataset = CustomTextDataset(\n",
    "        [tokenized_texts[i] for i in test_index],\n",
    "        df_cleaned['labels'].iloc[test_index].tolist(),\n",
    "        df_cleaned['core_count'].iloc[test_index].tolist(),\n",
    "        df_cleaned['emerging_count'].iloc[test_index].tolist()\n",
    "    )\n",
    "\n",
    "    # Initialize a fresh model for each fold with weighted loss\n",
    "    model = CustomRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n",
    "\n",
    "    # Trainer with early stopping callback\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=lambda p: {\n",
    "            'accuracy': accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1)),\n",
    "            'f1': classification_report(p.label_ids, np.argmax(p.predictions, axis=1), target_names=[str(i+1) for i in range(5)], output_dict=True)['weighted avg']['f1-score']\n",
    "        },\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_result = trainer.evaluate()\n",
    "    print(f\"Fold {fold} - Accuracy: {eval_result['eval_accuracy']}\")\n",
    "    print(f\"Fold {fold} - F1-Score: {eval_result['eval_f1']}\")\n",
    "\n",
    "    all_fold_results.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': eval_result['eval_accuracy'],\n",
    "        'f1': eval_result['eval_f1']\n",
    "    })\n",
    "\n",
    "# Display results for all folds\n",
    "for result in all_fold_results:\n",
    "    print(f\"Fold {result['fold']} - Accuracy: {result['accuracy']}, F1-Score: {result['f1']}\")\n",
    "\n",
    "# Calculate and display the average results\n",
    "avg_accuracy = np.mean([result['accuracy'] for result in all_fold_results])\n",
    "avg_f1 = np.mean([result['f1'] for result in all_fold_results])\n",
    "print(f\"\\nAverage Accuracy: {avg_accuracy}\")\n",
    "print(f\"Average F1-Score: {avg_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform hyperparameter tuning\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_space = {\n",
    "    'learning_rate': [2e-5, 3e-5, 4e-5],\n",
    "    'num_train_epochs': [3, 4, 5],\n",
    "    'per_device_train_batch_size': [8, 16, 32]\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_space):\n",
    "    print(f\"Running with parameters: {params}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = CustomTextDataset(\n",
    "        tokenized_texts,\n",
    "        df_cleaned['labels'].tolist(),\n",
    "        df_cleaned['core_count'].tolist(),\n",
    "        df_cleaned['emerging_count'].tolist()\n",
    "    )\n",
    "    test_dataset = CustomTextDataset(\n",
    "        tokenized_texts,\n",
    "        df_cleaned['labels'].tolist(),\n",
    "        df_cleaned['core_count'].tolist(),\n",
    "        df_cleaned['emerging_count'].tolist()\n",
    "    )\n",
    "\n",
    "    # Initialize a fresh model for each fold with weighted loss\n",
    "    model = CustomRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n",
    "\n",
    "    # Trainer with early stopping callback\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir='./results',  # Directory for storing logs and model checkpoints\n",
    "            num_train_epochs=params['num_train_epochs'],  # Train for 7 epochs\n",
    "            per_device_train_batch_size=params['per_device_train_batch_size'],  # Smaller batch size to manage GPU memory\n",
    "            per_device_eval_batch_size=16,  # Batch size for evaluation\n",
    "            warmup_steps=500,  # Number of warmup steps for learning rate scheduler\n",
    "            warmup_ratio=0.1,  # Alternatively, set warmup as a ratio of total steps\n",
    "            weight_decay=0.01,  # Weight decay for regularization\n",
    "            logging_dir='./logs',  # Directory for storing logs\n",
    "            logging_steps=50,  # Log model predictions every 50 steps\n",
    "            evaluation_strategy=\"epoch\",  # Evaluate model at the end of each epoch\n",
    "            learning_rate=params['learning_rate'],  # Higher initial learning rate\n",
    "            lr_scheduler_type='linear',  # Learning rate scheduler\n",
    "            fp16=True,  # Enable mixed precision training\n",
    "            gradient_accumulation_steps=2,  # Use gradient accumulation for larger effective batch size\n",
    "            save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "            load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "            metric_for_best_model=\"eval_loss\",  # Use evaluation loss to determine the best model\n",
    "            save_total_limit=2  # Limit the total amount of checkpoints; older ones are deleted\n",
    "        ),\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=lambda p: {\n",
    "            'accuracy': accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1)),\n",
    "            'f1': classification_report(p.label_ids, np.argmax(p.predictions, axis=1), target_names=[str(i+1) for i in range(5)], output_dict=True)['weighted avg']['f1-score']\n",
    "        },\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_result = trainer.evaluate()\n",
    "\n",
    "    # Save the best model\n",
    "    if eval_result['eval_accuracy'] > best_accuracy:\n",
    "        best_accuracy = eval_result['eval_accuracy']\n",
    "        best_params = params\n",
    "        \n",
    "    print(f\"Fold {fold} - Accuracy: {eval_result['eval_accuracy']}\")\n",
    "    print(f\"Fold {fold} - F1-Score: {eval_result['eval_f1']}\")\n",
    "\n",
    "    all_fold_results.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': eval_result['eval_accuracy'],\n",
    "        'f1': eval_result['eval_f1']\n",
    "    })\n",
    "\n",
    "    fold += 1\n",
    "    \n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best accuracy: {best_accuracy}\")\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "model = CustomRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n",
    "\n",
    "# Trainer with early stopping callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir='./results',  # Directory for storing logs and model checkpoints\n",
    "        num_train_epochs=best_params['num_train_epochs'],  # Train for 7 epochs\n",
    "        per_device_train_batch_size=best_params['per_device_train_batch_size'],  # Smaller batch size to manage GPU memory\n",
    "        per_device_eval_batch_size=16,  # Batch size for evaluation\n",
    "        warmup_steps=500,  # Number of warmup steps for learning rate scheduler\n",
    "        warmup_ratio=0.1,  # Alternatively, set warmup as a ratio of total steps\n",
    "        weight_decay=0.01,  # Weight decay for regularization\n",
    "        logging_dir='./logs',  # Directory for storing logs\n",
    "        logging_steps=50,  # Log model predictions every 50 steps\n",
    "        evaluation_strategy=\"epoch\",  # Evaluate model at the end of each epoch\n",
    "        learning_rate=best_params['learning_rate'],  # Higher initial learning rate\n",
    "        lr_scheduler_type='linear',  # Learning rate scheduler\n",
    "        fp16=True,  # Enable mixed precision training\n",
    "        gradient_accumulation_steps=2,  # Use gradient accumulation for larger effective batch size\n",
    "        save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "        load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "        metric_for_best_model=\"eval_loss\",  # Use evaluation loss to determine the best model\n",
    "        save_total_limit=2  # Limit the total amount of checkpoints; older ones are deleted\n",
    "    ),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=lambda p: {\n",
    "        'accuracy': accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1)),\n",
    "        'f1': classification_report(p.label_ids, np.argmax(p.predictions, axis=1), target_names=[str(i+1) for i in range(5)], output_dict=True)['weighted avg']['f1-score']\n",
    "    },\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
